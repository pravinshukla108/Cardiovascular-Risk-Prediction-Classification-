{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "GF8Ens_Soomf",
        "4_0_7-oCpUZd",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "BhH2vgX9EjGr",
        "rMDnDkt2B6du",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "P1XJ9OREExlT",
        "TIqpNgepFxVj",
        "OB4l2ZhMeS1U",
        "dJ2tPlVmpsJ0",
        "_-qAgymDpx6N",
        "pQ-RFyC0xOpZ",
        "C2i-_i-xxWQT",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "KH5McJBi2d8v",
        "-Kee-DAl2viO"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pravinshukla108/Cardiovascular-Risk-Prediction-Classification-/blob/main/Cardiovascular_Risk_Prediction_(Classification).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - ***Cardiovascular Risk Prediction***\n",
        "\n"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - Classification      \n",
        "##### **Contribution**    - Individual\n",
        "##### ***Name -***   ***PRAVIN KUMAR  SHUKLA***"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***This project used machine learning to predict 10-year CHD risk in patients from Framingham, Massachusetts, using a dataset of 3,400 records and 17 attributes. The data underwent thorough preprocessing, including handling missing values, outlier removal, and feature transformations.***\n",
        "\n",
        "***To address data imbalance, SMOTE and Tomek links were used. Feature scaling was applied for uniformity. The primary model, a tuned Neural Network, focused on recall to correctly identify CHD risk.***\n",
        "\n",
        "***In conclusion, this project showcased the power of machine learning in healthcare, accurately predicting CHD risk and emphasizing the potential for real-world impact. It highlighted the synergy between analytics and healthcare for better decision-making and patient care.***"
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[GitHub Link](https://https://github.com/pravinshukla108/Cardiovascular-Risk-Prediction-Classification-)"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***The objective of this research is to develop a predictive model using a dataset of approximately 3,400 records from residents of Framingham, Massachusetts.***\n",
        "\n",
        "\n",
        "***The model aims to predict the 10-year risk of future coronary heart disease (CHD) based on 17 attributes, which encompass demographic, behavioral, and medical factors. The problem statement involves analyzing the dataset to understand the complex interactions between these attributes and their impact on CHD risk, ultimately enabling better risk assessment and preventive measures for individuals in the Framingham community.***"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## Import Libraries\n",
        "\n",
        "# Data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Data visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Warning handling (Filter out warnings to improve code readability)\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Statistical modeling and analysis\n",
        "import statsmodels.api as sm\n",
        "from statsmodels.stats.outliers_influence import variance_inflation_factor  # Detect multicollinearity\n",
        "\n",
        "# Data preprocessing and splitting\n",
        "from sklearn.model_selection import train_test_split  # Split data into training and testing sets\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler  # Data scaling\n",
        "\n",
        "# Handling imbalanced data\n",
        "from imblearn.combine import SMOTETomek  # Combining over-sampling and under-sampling for imbalanced datasets\n",
        "\n",
        "## Machine learning models and evaluation metrics\n",
        "\n",
        "# Linear & Logistic regression model\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "\n",
        "# Hyperparameter tuning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "\n",
        "# Gaussian Naive Bayes model\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "# Support Vector Machine model\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "# Multi-layer Perceptron Neural Network model\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "# Custom model evaluation metrics\n",
        "from sklearn.metrics import make_scorer, recall_score, f1_score, precision_score\n",
        "\n",
        "# XGBoost Random Forest Classifier\n",
        "from xgboost import XGBRFClassifier\n",
        "\n",
        "# Feature selection using chi-squared statistic\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from sklearn.inspection import permutation_importance\n"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount('/pravindrive', force_remount=True)\n",
        "file_path = '/pravindrive/MyDrive/Colab Notebooks/Cardiovascular Risk Prediction(Classification)/data_cardiovascular_risk.csv'\n",
        "df= pd.read_csv(file_path)\n"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head().T\n"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.tail().T"
      ],
      "metadata": {
        "id": "24JnSC-twF8b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "print(f'The number of rows in the data is : {df.shape[0]}')\n",
        "print(f'The number of columns in the data is : {df.shape[1]}')\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "# Create a copy of the DataFrame to keep the original data intact\n",
        "om = df.copy()\n",
        "\n",
        "# Calculate the number of duplicate rows in the copied DataFrame\n",
        "duplicate_om = om.duplicated().sum()\n",
        "\n",
        "# Print the count of duplicate rows\n",
        "print('Duplicate rows in cardiovascular dataset:', duplicate_om)\n"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "print(f\"Missing Values in each column\"+\"\\n\" + \"*%-_-%*\"*5)\n",
        "print(om.isnull().sum())"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.heatmap(om.isnull(), cmap='flare')\n",
        "plt.title('Visualization of Missing Data')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's a summary of what I know about the dataset:\n",
        "\n",
        "1. **Dataset Size:** The dataset consists of 3,390 rows and 17 columns. The dependent variable is \"TenYearCHd,\" which is likely the target variable for a predictive model.\n",
        "\n",
        "2. **Duplicate Rows:** No duplicate rows are found in the dataset. Each row is unique.\n",
        "\n",
        "3. **Missing Values:** The dataset contains missing values in the following columns:\n",
        "      *  education\n",
        "      *  CigsPerDay\n",
        "      *  BPMeds\n",
        "      *  totChol\n",
        "      *  BMI\n",
        "      *  glucose\n",
        "\n",
        "\n",
        "  ***There are a total of 510 missing values across these columns.***\n",
        "\n",
        "4. **Columns for Conversion:** Two columns,\"is_smoking\" ** and \"sex\" need to be converted into numeric format for modeling purposes. This conversion is likely required because some machine learning algorithms require numeric input features, and these columns may currently be represented as categorical variables."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "om.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "om.describe(include = 'all').T"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Demographic:**\n",
        "\n",
        "1. `id`: Identifier for each patient in the dataset.\n",
        "\n",
        "2. `age`: Age of the patient in years.\n",
        "\n",
        "3. `education`: Education level of the patient (categorical or numerical).\n",
        "\n",
        "4. `sex`: Gender of the patient (binary: 0 for female, 1 for male).\n",
        "\n",
        "**Behavior:**\n",
        "\n",
        "5. `is_smoking`: Whether the patient is a current smoker (binary: 0 for non-smoker, 1 for smoker).\n",
        "\n",
        "6. `cigsPerDay`: Average number of cigarettes smoked per day (continuous).\n",
        "\n",
        "**Medical History:**\n",
        "\n",
        "7. `BPMeds`: Whether the patient takes blood pressure medication (binary: 0 for not taking medication, 1 for taking medication).\n",
        "\n",
        "8. `prevalentStroke`: History of stroke for the patient (binary: 0 for no stroke history, 1 for stroke history).\n",
        "\n",
        "9. `prevalentHyp`: History of hypertension for the patient (binary: 0 for no hypertension, 1 for hypertension).\n",
        "\n",
        "10. `diabetes`: Whether the patient has diabetes (binary: 0 for no diabetes, 1 for diabetes).\n",
        "\n",
        "**Medical (Current):**\n",
        "\n",
        "11. `totChol`: Total cholesterol level of the patient in mg/dL (continuous).\n",
        "\n",
        "12. `sysBP`: Systolic blood pressure level of the patient in mmHg (continuous).\n",
        "\n",
        "13. `diaBP`: Diastolic blood pressure level of the patient in mmHg (continuous).\n",
        "\n",
        "14. `BMI`: Body Mass Index (BMI) of the patient (continuous).\n",
        "\n",
        "15. `heartRate`: Resting heart rate of the patient in beats per minute (continuous).\n",
        "\n",
        "16. `glucose`: Blood glucose level of the patient in mg/dL (continuous).\n",
        "\n",
        "**Target Variable:**\n",
        "\n",
        "17. `TenYearCHD`: Ten-year risk of developing coronary heart disease (CHD) (binary: 0 for no risk, 1 for risk).\n",
        "\n"
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable.\n",
        "# Get the count of unique values for each column\n",
        "unique_counts = om.nunique()\n",
        "\n",
        "# Print the count of unique values for each column\n",
        "print(\"Count of Unique Values for Each Column:\"+\"\\n\")\n",
        "print(\"@(0_0)@ \" * 5+\"\\n\")\n",
        "print(unique_counts)\n",
        "\n",
        "\n",
        "# Add a separator for clarity\n",
        "print(\"\\n\"+\"@(0_0)@ \" * 5)\n",
        "\n"
      ],
      "metadata": {
        "id": "zms12Yq5n-jE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***3. Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "\n",
        "# Checking for null values in each column and sorting by the number of null values (descending)\n",
        "null_values = om.isnull().sum().sort_values(ascending=False)\n",
        "null_values\n",
        "\n"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to show columns with missing values\n",
        "def show_missing():\n",
        "    missing = om.columns[om.isnull().any()].tolist()\n",
        "    return missing\n",
        "# Calculate and display the percentage of missing values for selected columns\n",
        "missing_percentage = round(om[show_missing()].isnull().sum().sort_values(ascending=False) / len(om) * 100, 2)\n",
        "missing_percentage\n"
      ],
      "metadata": {
        "id": "ilE--Za4dhHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize the distribution of selected numerical columns with missing values\n",
        "\n",
        "COLOR=['dodgerblue','goldenrod','red','mediumpurple','violet']\n",
        "numerical_columns_with_missing = ['glucose', 'totChol', 'BMI', 'heartRate', 'cigsPerDay']\n",
        "\n",
        "for i, column in enumerate(numerical_columns_with_missing):\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.distplot(om[column], color=COLOR[i % len(COLOR)])  # Use COLOR list with index i\n",
        "    plt.title(f'Distribution of {column}')\n",
        "\n",
        "plt.show()  # Display all figures\n"
      ],
      "metadata": {
        "id": "nB6ir8XbdrOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop the 'id' column as it is typically not needed for analysis\n",
        "om = om.drop(columns='id')\n",
        "\n",
        "# Categorize categorical and numerical variables\n",
        "categorical = [var for var in om.columns if om[var].dtype == 'object']\n",
        "numerical = [var for var in om.columns if om[var].dtype != 'object']\n",
        "\n",
        "# Identify discreet features with fewer than 20 unique values\n",
        "discreet = [var for var in om.columns if len(om[var].unique()) < 20]\n",
        "\n",
        "# Identify continuous features\n",
        "continuous = [var for var in om.columns if var not in discreet and var != 'TenYearCHD']\n",
        "print('There are {} categorical variables'.format(len(categorical)))\n",
        "print('There are {} numerical variables'.format(len(numerical)))\n",
        "print('There are {} discreet variables'.format(len(discreet)))\n",
        "print(categorical)\n",
        "\n",
        "print(numerical)\n",
        "print(discreet)"
      ],
      "metadata": {
        "id": "kwfh_a7fd16c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Handling missing values in categorical variables\n",
        "om['BPMeds'] = om['BPMeds'].fillna(om['BPMeds'].mode()[0])\n",
        "om['education'] = om['education'].fillna(om['education'].mode()[0])\n",
        "\n",
        "# Handling missing values in the continuous variable 'cigsPerDay'\n",
        "# Calculate mean and median of 'cigsPerDay'\n",
        "cigsPerDay_mean = om['cigsPerDay'].mean().round(0)\n",
        "cigsPerDay_median = om['cigsPerDay'].median()\n",
        "\n",
        "# Check for missing values where 'is_smoking' is 'YES'\n",
        "missing_cigsPerDay = om[om['cigsPerDay'].isnull() & (om['is_smoking'] == 'YES')]\n",
        "\n",
        "# Impute missing values in 'cigsPerDay' with the median for 'is_smoking' = 'YES'\n",
        "median_cigsPerDay_smoking = om[om['is_smoking'] == 'YES']['cigsPerDay'].median()\n",
        "om['cigsPerDay'] = om['cigsPerDay'].fillna(median_cigsPerDay_smoking)\n",
        "\n",
        "# Handle missing entries where 'is_smoking' is 'YES' and 'cigsPerDay' is 0\n",
        "smokers_with_zero_cigsPerDay = om[(om['is_smoking'] == 'YES') & (om['cigsPerDay'] == 0)]\n",
        "\n",
        "# Handling missing values in 'totChol', 'BMI', and 'heartRate'\n",
        "totChol_mean = om['totChol'].mean()\n",
        "totChol_median = om['totChol'].median()\n",
        "om['totChol'] = om['totChol'].fillna(totChol_median)\n",
        "\n",
        "BMI_mean = om['BMI'].mean()\n",
        "BMI_median = om['BMI'].median()\n",
        "om['BMI'] = om['BMI'].fillna(BMI_median)\n",
        "\n",
        "heartRate_mean = om['heartRate'].mean()\n",
        "heartRate_median = om['heartRate'].median()\n",
        "om['heartRate'] = om['heartRate'].fillna(heartRate_median)\n",
        "\n",
        "\n",
        "# Check for remaining missing/null values in the dataset\n",
        "missing_values_after_imputation = om.isnull().sum()\n",
        "missing_values_after_imputation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SdwEVX2iAYFo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "om.columns"
      ],
      "metadata": {
        "id": "LAd9v8UNk5ff"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The code performs the following data preparation steps:\n",
        "\n",
        "1. **Handling Missing Values:**\n",
        "   - Identifies and sorts null values by the number of missing values.\n",
        "   - Calculates and displays the percentage of missing values for selected columns.\n",
        "   - Visualizes the distribution of selected numerical columns with missing values.\n",
        "   - Imputes missing numerical values with the median and missing categorical values with the mode.\n",
        "   - Confirms that all missing values are handled after imputation.\n",
        "\n",
        "2. **Data Transformation:**\n",
        "   - Drops the 'id' column, which is typically not needed for analysis.\n",
        "\n",
        "3. **Variable Categorization:**\n",
        "   - Identifies categorical and numerical variables.\n",
        "   - Separates discreet features with fewer than 20 unique values.\n",
        "   - Identifies continuous features.\n",
        "\n",
        "These manipulations prepare the dataset for analysis, ensuring data integrity and appropriate categorization of variables. The insights obtained include addressing missing values, visualizing data distribution, and categorizing variables effectively."
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "\n",
        "# Data visualization - 3D Pie chart for TenYearCHD\n",
        "ten_year_chd_counts = om['TenYearCHD'].value_counts()\n",
        "labels = ['No Risk (0)', 'Risk (1)']\n",
        "colors = ['#6ACCBE', '#F96E46']\n",
        "explode = (0.22, 0)  # Explode the first slice (No Risk)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.pie(ten_year_chd_counts, labels=labels, colors=colors, autopct='%1.1f%%', startangle=142,\n",
        "       explode=explode, shadow=True)\n",
        "\n",
        "plt.title('Distribution of TenYearCHD')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***I used a pie chart to show the TenYearCHD distribution because it's a straightforward way to display the proportions of people with and without TenYearCHD (labeled as 1 and 0). It offers a clear visual of the relative sizes of these two categories, making it easy to understand the dataset's target variable distribution at a glance.***"
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***The data shows an imbalanced distribution in the TenYearCHD variable, with 85% of 0 values and 15% of 1 values.***"
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Identified data imbalance, which may impact model efficiency. Recommends applying sampling techniques to address this imbalance, potentially improving model performance and predictive accuracy.***"
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "# Grouping the data to count the occurrences of 'is_smoking' values for each 'TenYearCHD' class\n",
        "tenyear_smoking = om.groupby('TenYearCHD')['is_smoking'].value_counts().unstack(0)\n",
        "\n",
        "# Calculating the percentage of smokers and non-smokers for each TenYearCHD class\n",
        "percentage_df = tenyear_smoking.divide(tenyear_smoking.sum(axis=1), axis=0) * 100\n",
        "\n",
        "# Define custom colors for the bars\n",
        "colors = ['#FF5733', '#33FF57']  # You can change these colors\n",
        "\n",
        "# Creating a bar chart with custom colors\n",
        "percentage_df.plot(kind='bar', color=colors)\n",
        "plt.ylim(0, 100)\n",
        "plt.ylabel('Percentage')\n",
        "plt.xlabel('Smoking Status (0 = Non-smoker, 1 = Smoker)')\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', linestyle='-', alpha=0.4)\n",
        "plt.title('Percentage of Smokers and Non-Smokers by TenYearCHD')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Utilized a bar chart to illustrate the comparison of CHD risk percentages concerning smoking habits.***"
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Smokers face a higher CHD risk compared to non-smokers, although the disparity is not substantial.***"
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Positive Business Impact: Identifying the high CHD risk in smokers can drive demand for smoking cessation solutions, benefiting the healthcare and related industries. This insight may boost the need for programs and products aiding individuals in quitting smoking, reducing CHD susceptibility.***"
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "\n",
        "# Create a figure with a specified background color\n",
        "fig = plt.figure(figsize=(12, 8))\n",
        "fig.patch.set_facecolor(\"#FF00FF\")\n",
        "\n",
        "\n",
        "# Chart - 3: Visualizing the relationship between diabetes and glucose using a box plot\n",
        "sns.boxplot(x='diabetes', y='glucose', data=om, palette='magma')  # Set the color palette\n",
        "plt.title('Diabetes vs. Glucose Distribution', color='blue')  # Set the title color\n",
        "plt.xlabel('Diabetes (0 = No, 1 = Yes)', color='darkred')  # Set the x-axis label color\n",
        "plt.ylabel('Glucose Level', color='green')  # Set the y-axis label color\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "   ***I chose a box plot because it's great for comparing glucose levels between individuals with and without diabetes. It helps us see differences in the central tendency, spread, and any outliers.***"
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Insights from the Chart:**\n",
        "   - People with diabetes have higher and more variable glucose levels (seen from the wider box and more outliers).\n",
        "   - People without diabetes have a more concentrated and less variable glucose distribution."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "***The insight can lead to increased demand for diabetes management products, services, and research. It's good for the healthcare industry and offers opportunities for better care. There might be some increased costs, but overall, it's a positive impact.***\n",
        "\n",
        "\n",
        "   "
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "# Visualizing the relationship between sex and TenYearCHD using a grouped bar chart\n",
        "# Calculate the counts of TenYearCHD by sex and create a dataframe\n",
        "tenyear_sex = om.groupby('TenYearCHD')['sex'].value_counts().unstack(0)\n",
        "\n",
        "# Calculate the percentage of TenYearCHD by sex\n",
        "percentage_df = tenyear_sex.divide(tenyear_sex.sum(axis=1), axis=0) * 100\n",
        "\n",
        "# Define the colors for bars and edges\n",
        "bar_colors = ['#FFA07A', '#6495ED']\n",
        "edge_colors = ['red', 'blue']\n",
        "\n",
        "# Plot a grouped bar chart with colorful edges\n",
        "ax = percentage_df.plot(kind='bar', color=bar_colors, edgecolor=edge_colors)\n",
        "plt.ylim(0, 100)\n",
        "plt.grid(axis='y', linestyle=':', alpha=0.6)\n",
        "plt.ylabel('Percentage', color='green')\n",
        "plt.xlabel('Sex (0 = Female, 1 = Male)', color='blue')\n",
        "plt.xticks(rotation=0)\n",
        "\n",
        "# Set colorful grid and axis edges\n",
        "ax.spines['bottom'].set_color('lime')\n",
        "ax.spines['top'].set_color('purple')\n",
        "ax.spines['right'].set_color('brown')\n",
        "ax.spines['left'].set_color('blue')\n",
        "ax.tick_params(axis='x', colors='blue')\n",
        "ax.tick_params(axis='y', colors='green')\n",
        "\n",
        "plt.title('TenYearCHD by Sex', color='darkred')\n",
        "plt.legend(title='TenYearCHD', labels=['No CHD', 'CHD'])\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***I created a grouped bar chart to compare CHD risk percentages by gender. It helps easily distinguish between \"No CHD\" and \"CHD\" categories for each gender. Color-coded bars and colorful edges enhance the visual appeal. The y-axis shows percentages, and the x-axis represents genders. Custom colors make the chart more visually appealing.***"
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Men show a higher CHD risk percentage for the next decade..***"
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Positive Business Impact: Recognizing the higher CHD risk in males opens up opportunities in the healthcare industry. Tailoring gender-specific healthcare solutions can drive business growth.***\n",
        "\n",
        "***Negative Business Impact: However, the increased risk in males may also lead to higher healthcare costs for businesses, impacting their financial performance.***"
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "\n",
        "#visualizing the relationship between education and TenYearCHD.\n",
        "\n",
        "# Calculate the counts of TenYearCHD by education and create a dataframe\n",
        "tenyear_education = om.groupby('TenYearCHD')['education'].value_counts().unstack(0)\n",
        "\n",
        "# Calculate the percentage of TenYearCHD by education\n",
        "percentage_df = tenyear_education.divide(tenyear_education.sum(axis=1), axis=0) * 100\n",
        "\n",
        "# Define the colors for bars and edges\n",
        "bar_colors = ['#FFA07A', '#6495ED']\n",
        "edge_colors = ['red', 'blue']\n",
        "\n",
        "# Plot a stacked bar chart with colorful edges\n",
        "ax = percentage_df.plot(kind='bar', stacked=True, color=bar_colors, edgecolor=edge_colors)\n",
        "plt.ylim(0, 100)\n",
        "plt.ylabel('Percentage', color='green')\n",
        "plt.xlabel('Education Level', color='blue')\n",
        "plt.xticks(rotation=0)\n",
        "\n",
        "# Display numbers for each stacked bar\n",
        "for p in ax.patches:\n",
        "    width, height = p.get_width(), p.get_height()\n",
        "    x, y = p.get_xy()\n",
        "    ax.annotate(f'{height:.1f}%', (x + width / 2, y + height / 2), ha='center', va='center', fontsize=10)\n",
        "\n",
        "# Set colorful grid and axis edges\n",
        "ax.spines['bottom'].set_color('purple')\n",
        "ax.spines['top'].set_color('orange')\n",
        "ax.spines['right'].set_color('magenta')\n",
        "ax.spines['left'].set_color('blue')\n",
        "ax.tick_params(axis='x', colors='blue')\n",
        "ax.tick_params(axis='y', colors='green')\n",
        "\n",
        "plt.title('TenYearCHD by Education Level', color='darkred')\n",
        "plt.legend(title='TenYearCHD', labels=['No CHD', 'CHD'])\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***The stacked bar chart was chosen to visualize the relationship between education levels and TenYearCHD. Because It effectively displays the distribution of CHD risk across different education levels.***"
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***The chart shows how the risk of CHD varies among individuals with varying education levels.***"
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "***Positive Business Impact: Tailoring healthcare and education programs to different education levels can help reduce CHD risk.***\n",
        "\n",
        "\n",
        "***Negative Business Impact: Focusing on specific education levels might strain resources and budgets, potentially affecting financial performance.***"
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code for the realtionship between prevalentHyp and TenYearCHD.\n",
        "\n",
        "# Group data and calculate percentages\n",
        "tenyear_hyp = om.groupby('TenYearCHD')['prevalentHyp'].value_counts().unstack(0)\n",
        "pct_df = tenyear_hyp.divide(tenyear_hyp.sum(axis=1), axis=0) * 100\n",
        "edge_colors = ['red', 'blue']\n",
        "# Create a bar plot\n",
        "ax = pct_df.plot(kind='bar', color=['skyblue', 'salmon'],edgecolor=edge_colors)\n",
        "\n",
        "# Customize the plot\n",
        "plt.ylim(0, 100)\n",
        "plt.ylabel('Percentage')\n",
        "plt.xlabel('Prevalent Hypertension (0 = No, 1 = Yes)')\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', linestyle=':', alpha=0.5)\n",
        "\n",
        "# Add legend and labels\n",
        "plt.legend(['No CHD', 'CHD'], title='TenYearCHD')\n",
        "plt.title('Relationship Between Prevalent Hypertension and Ten-Year CHD')\n",
        "\n",
        "# Set colorful grid and axis edges\n",
        "ax.spines['bottom'].set_color('purple')\n",
        "ax.spines['top'].set_color('orange')\n",
        "ax.spines['right'].set_color('magenta')\n",
        "ax.spines['left'].set_color('blue')\n",
        "ax.tick_params(axis='x', colors='blue')\n",
        "ax.tick_params(axis='y', colors='green')\n",
        "\n",
        "# Annotate the bars with the exact percentage values\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height():.2f}%', (p.get_x() + p.get_width() / 2, p.get_height()), ha='center')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***I employed a bar chart to visually depict the fluctuations in the percentage of CHD risk associated with prevalent hypertension.***"
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Hypertensive patients are at a heightened risk of developing coronary heart disease.***"
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***This information can be a vital awareness tool for individuals with diabetes, motivating them to take preventive measures to safeguard their well-being.***"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code for the realtionship between BPMeds and TenYearCHD.\n",
        "\n",
        "# Group data and calculate percentages\n",
        "tenyear_BP = om.groupby('TenYearCHD')['BPMeds'].value_counts().unstack(0)\n",
        "pct_df = tenyear_BP.divide(tenyear_BP.sum(axis=1), axis=0) * 100\n",
        "\n",
        "# Create a bar plot\n",
        "ax = pct_df.plot(kind='bar', color=['skyblue', 'salmon'], edgecolor='black', zorder=2)\n",
        "\n",
        "# Customize the plot\n",
        "plt.ylim(0, 100)\n",
        "plt.ylabel('Percentage')\n",
        "plt.xlabel('BPMeds (0 = No, 1 = Yes)')\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', linestyle=':', alpha=0.9)\n",
        "\n",
        "# Add edgecolor to axes\n",
        "ax.spines['left'].set_color('skyblue')\n",
        "ax.spines['bottom'].set_color('salmon')\n",
        "ax.spines['top'].set_color('orange')\n",
        "ax.spines['right'].set_color('magenta')\n",
        "\n",
        "# Add legend and labels\n",
        "plt.legend(['No CHD', 'CHD'], title='TenYearCHD')\n",
        "plt.title('Relationship Between Medication for High Blood Pressure and Ten-Year CHD', color='red')\n",
        "\n",
        "# Annotate the bars with the exact percentage values\n",
        "for p in ax.patches:\n",
        "    ax.annotate(f'{p.get_height():.2f}%', (p.get_x() + p.get_width() / 2, p.get_height()), ha='center')\n",
        "\n",
        "\n",
        "# Show the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "***The chosen chart is a grouped bar chart, used to visualize the relationship between 'BPMeds' (Blood Pressure Medication) and 'TenYearCHD' (Ten-Year Coronary Heart Disease Risk).***\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "***The chart shows that medication for high blood pressure can help reduce the risk of CHD. The percentage of people with CHD is lower among those who take medication for high blood pressure compared to those who do not.***\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "***The insights gained from the chart can help create a positive business impact by encouraging people to take medication for high blood pressure. This can help reduce the risk of CHD and other cardiovascular diseases. There are no insights that lead to negative growth.***\n",
        "\n",
        "\n",
        "\n",
        "**Business Impact :-**\n",
        "   - The chart can inform healthcare providers and pharmaceutical companies about the relationship between blood pressure medication and CHD risk.\n",
        "   - Encouraging individuals with a higher risk to consider medication may improve disease management and reduce CHD prevalence.\n",
        "   - No negative insights, as encouraging medication for at-risk individuals is a positive health intervention."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code for the realtionship between sex and sysBP.\n",
        "\n",
        "\n",
        "# Custom color palette\n",
        "colors = ['#FF6F61', '#6B5B95']\n",
        "\n",
        "# Set style and figure size\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.figure(figsize=(14, 8))\n",
        "\n",
        "# Create a violin plot with custom colors\n",
        "sns.violinplot(x='sex', y='sysBP', data=om, palette=colors)\n",
        "\n",
        "# Customize labels, title, grid, and axes\n",
        "plt.xlabel('Sex (0 = Female, 1 = Male)', fontsize=14, color='navy')\n",
        "plt.ylabel('Systolic Blood Pressure', fontsize=14, color='darkred')\n",
        "plt.title('Relationship Between Sex and Systolic Blood Pressure', fontsize=16, color='forestgreen')\n",
        "\n",
        "# Customize grid and axis colors\n",
        "grid_color = 'red'\n",
        "axis_colors = ['purple', 'darkorange', 'lightblue', 'coral']\n",
        "\n",
        "plt.grid(axis='both', linestyle='-.', alpha=0.8, color=grid_color)\n",
        "\n",
        "ax = plt.gca()\n",
        "\n",
        "for spine, color in zip(ax.spines.values(), axis_colors):\n",
        "    spine.set_color(color)\n",
        "\n",
        "# Customize numerical values on the axes\n",
        "ax.tick_params(axis='x', colors='red', labelsize=12)\n",
        "ax.tick_params(axis='y', colors='green', labelsize=12)\n",
        "# Show the plot\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***The chart chosen is a violin plot. Violin plots are effective for visualizing the distribution of data, especially when comparing two categories like 'sex' in this case. The use of custom colors and style elements enhances the visual appeal and interpretability of the plot.***"
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***The violin plot reveals the distribution of systolic blood pressure for both females and males.***\n",
        "   - ***It shows that the distribution for females is more spread out, and the median systolic blood pressure is higher for females compared to males.\n",
        "   The distribution of systolic blood pressure differs between genders. Females have a wider spread and a higher median systolic blood pressure compared to males.***\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Gender-specific considerations in blood pressure management can lead to more tailored healthcare interventions without negative implications.***\n"
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code for the relationship between sex and BMI\n",
        "\n",
        "# Custom color palette\n",
        "colors = ['lightgreen', 'cyan']\n",
        "\n",
        "# Set style and figure size\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.figure(figsize=(12, 8))\n",
        "\n",
        "# Create a violin plot with custom colors\n",
        "sns.violinplot(x='sex', y='BMI', data=om, palette=colors)\n",
        "\n",
        "# Customize labels, title, grid, and axes\n",
        "plt.xlabel('Sex (0 = Female, 1 = Male)', fontsize=14, color='navy')\n",
        "plt.ylabel('Body Mass Index (BMI)', fontsize=14, color='darkred')\n",
        "plt.title('Relationship Between Sex and BMI', fontsize=16, color='forestgreen')\n",
        "\n",
        "# Customize grid and axis colors\n",
        "grid_color = 'red'\n",
        "axis_colors = ['purple', 'darkorange', 'lightblue', 'coral']\n",
        "\n",
        "plt.grid(axis='both', linestyle='--', alpha=0.5, color=grid_color)\n",
        "ax = plt.gca()\n",
        "\n",
        "for spine, color in zip(ax.spines.values(), axis_colors):\n",
        "    spine.set_color(color)\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***I chose the violin plot as it effectively shows the BMI distribution for both males and females. It provides a clear view of data distribution and comparisons between the two gender categories.***"
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Most males have BMI values slightly above the healthy range of 20 to 25, while most females maintain BMIs below 25. However, the presence of significant outliers is notable among females, with some reaching values between 55 and 60.***"
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Overall, males tend to have slightly higher average BMI scores than females. However, a larger percentage of females fall within the healthy BMI range. The presence of outliers among females indicates a subgroup with elevated risks related to weight-related health issues.***"
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code for the realtionship between diabetes and TenYearCHD.\n",
        "# Calculate the counts of TenYearCHD by diabetes and create a dataframe\n",
        "tenyear_diabetes = om.groupby('TenYearCHD')['diabetes'].value_counts().unstack(0)\n",
        "\n",
        "# Calculate the percentage of TenYearCHD by diabetes\n",
        "percentage_df = tenyear_diabetes.divide(tenyear_diabetes.sum(axis=1), axis=0) * 100\n",
        "\n",
        "# Define the colors for bars and edges\n",
        "bar_colors = ['#FFA07A', '#6495ED']\n",
        "edge_colors = ['red', 'blue']\n",
        "\n",
        "# Plot a stacked bar chart with colorful edges\n",
        "ax = percentage_df.plot(kind='bar', stacked=True, color=bar_colors, edgecolor=edge_colors , zorder=2)\n",
        "plt.ylim(0, 130)\n",
        "\n",
        "plt.ylabel('Percentage', color='green')\n",
        "plt.xlabel('TenYearCHD', color='blue')\n",
        "plt.xticks(rotation=0)\n",
        "plt.grid(axis='y', linestyle='-.', alpha=0.9)\n",
        "# Display numbers for each stacked bar\n",
        "for p in ax.patches:\n",
        "    width, height = p.get_width(), p.get_height()\n",
        "    x, y = p.get_xy()\n",
        "    ax.annotate(f'{height:.1f}%', (x + width / 2, y + height / 2), ha='center', va='center', fontsize=10)\n",
        "\n",
        "# Set colorful grid and axis edges\n",
        "ax.spines['bottom'].set_color('purple')\n",
        "ax.spines['top'].set_color('orange')\n",
        "ax.spines['right'].set_color('magenta')\n",
        "ax.spines['left'].set_color('blue')\n",
        "ax.tick_params(axis='x', colors='blue')\n",
        "ax.tick_params(axis='y', colors='green')\n",
        "\n",
        "plt.title('Relationship Between Diabetes and Ten-Year CHD', color='darkred')\n",
        "plt.legend(['No Diabetes', 'Diabetes'], title='Diabetes', loc='upper right')\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***I chose a stacked bar chart to visualize the relationship between diabetes and TenYearCHD, providing a clear comparison of the two categories within different levels of TenYearCHD.***"
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Individuals with diabetes have a significantly elevated 10-year risk of developing coronary heart disease (CHD).***"
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***This information can serve as a valuable awareness tool for individuals with diabetes, encouraging them to take necessary preventive measures to safeguard their heart health.***"
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code for the raeltionship between sex and cigsPerDay.\n",
        "# Group data and create a dataframe\n",
        "cigs_sex_data = om.groupby('sex')['cigsPerDay'].value_counts().unstack(0).plot.bar(figsize=(12,6))\n",
        "\n",
        "\n",
        "# Create a bar plot\n",
        "cigs_sex_data.plot(kind='bar')\n",
        "plt.grid(axis='both', linestyle='-.', alpha=0.5, color='#FAD6A5')\n",
        "\n",
        "# Customize labels and title\n",
        "plt.xlabel('cigsPerDay', fontsize=12, color='blue')\n",
        "plt.ylabel('Count', fontsize=12, color='green')\n",
        "plt.title('Relationship Between Sex and Cigarettes Per Day', fontsize=14, color='darkred')\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Utilized a bar chart to illustrate the comparison of the number of cigarettes smoked per day across different genders.***"
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Females predominantly consist of non-smokers; however, in cases where the number of cigarettes per day ranges from 5 to 15, females surpass males.***"
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  visualization code(Distribution of Education levels)\n",
        "# Setting labels for items in Chart\n",
        "education = om[\"education\"].value_counts()\n",
        "\n",
        "# Pie Chart\n",
        "om[\"education\"].value_counts().plot.pie(colors= sns.color_palette('pastel'),legend=True,autopct = '%1.1f%%',\n",
        "                                              wedgeprops = {'linewidth': 2,'antialiased': True})\n",
        "# draw circle\n",
        "centre_circle = plt.Circle((0, 0), 0.70, fc='white')\n",
        "fig = plt.gcf()\n",
        "\n",
        "# Adding Circle in Pie chart\n",
        "fig.gca().add_artist(centre_circle)\n",
        "\n",
        "# # Adding Title of chart\n",
        "plt.title('Education levels')\n",
        "\n",
        "# Displaying Chart\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have plotted donut chart to visualize the distribution of education levels among individuals being evaluated for their risk of cardiovascular disease.This type of chart can help to compare individual categories or dimensions to the larger whole, just like a pie chart, but with a couple of advantages. Donut charts can make it easier for us to compare individual dimensions."
      ],
      "metadata": {
        "id": "_AJ-umLByvWL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this dataset most of the population have Education Level - 1 with 42.1% and least education level 4 with 11.3% ."
      ],
      "metadata": {
        "id": "6ZC1FGFgyvWM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here"
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.title(\"Glucose vs Age\", color='blue', fontsize=15)  # Set title color and font size\n",
        "\n",
        "\n",
        "sns.lineplot(data=om, x='age', y='glucose', palette=\"orange\")\n",
        "\n",
        "plt.xlabel(\"Age\", color='green', fontsize=12)  # Set x-axis label color and font size\n",
        "plt.ylabel(\"Glucose\", color='purple', fontsize=12)  # Set y-axis label color and font size\n",
        "plt.xticks(color='red')  # Set x-axis tick color\n",
        "plt.yticks(color='orange')  # Set y-axis tick color\n",
        "plt.grid(True, linestyle='-.', alpha=0.9)  # Add grid lines with specified linestyle and transparency\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***I Chose a line plot to display the relationship between 'Age' and 'Glucose' continuously. It's great for showing trends over time, depicting how glucose levels change with age.***"
      ],
      "metadata": {
        "id": "ihBfru2wAwpl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ***The chart shows how glucose levels increase with age.***\n",
        "- ***Glucose is a type of sugar in the blood that gives energy.***\n",
        "- ***High glucose levels can cause diabetes and other health problems.***\n",
        "- ***Older people should check their glucose levels and live a healthy lifestyle.***"
      ],
      "metadata": {
        "id": "eOeYU28sAwpm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Understanding the age-glucose correlation benefits healthcare planning but could strain resources due to increased diabetes cases in older age groups. It demands targeted health programs for prevention and might require adjustments in insurance policies to manage higher risks associated with age-related health issues.***"
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "\n",
        "# Define the figure size\n",
        "plt.figure(figsize=(14, 10))\n",
        "\n",
        "# Create the correlation heatmap\n",
        "correlation_matrix = om.corr()\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='viridis', fmt='.2f', linewidths=0.5)\n",
        "\n",
        "# Add a title\n",
        "plt.title('Correlation Heatmap')\n",
        "\n",
        "# Customize x and y axis labels\n",
        "plt.xticks(rotation=45)  # Rotate x-axis labels for better readability\n",
        "plt.yticks(rotation=0)   # Keep y-axis labels horizontal\n",
        "\n",
        "# Display the heatmap\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***I used a correlation heatmap to visualize the relationships between various features.***"
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Systolic BP and Diastolic BP, as well as Systolic BP and Prevalent Hypertension, display noteworthy correlations.***"
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Pair Plot visualization code\n",
        "sns.set(style=\"ticks\", color_codes=True)\n",
        "pair_plot = sns.pairplot(om, hue=\"TenYearCHD\", palette=\"Set2\")\n",
        "pair_plot._legend.remove()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***I employed a pairplot to visualize the interrelationships among multiple variables in the dataset.***"
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***We noticed strong correlations between the following pairs of variables: Systolic BP - Diastolic BP, Systolic BP - Prevalent Hypertension, and Diastolic BP - Prevalent Hypertension.***"
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Null Hypothesis (H0):*** *\"There is no relationship between cigsPerDay and TenYearCHD.\"*\n",
        "\n",
        "***Alternative Hypothesis (H1 or Ha):*** *\"There is a relationship between cigsPerDay and TenYearCHD.\"*"
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "# Define the null and alternative hypotheses\n",
        "null_hypothesis = 'There is no relationship between cigsPerDay and TenYearCHD'\n",
        "alt_hypothesis = 'There is a relationship between cigsPerDay and TenYearCHD'\n",
        "\n",
        "# Perform linear regression\n",
        "x = sm.add_constant(om['cigsPerDay'])\n",
        "y = om['TenYearCHD']\n",
        "model = sm.OLS(y, x).fit()\n",
        "\n",
        "# Print regression summary\n",
        "print(model.summary())\n",
        "\n",
        "# Extract the p-value\n",
        "p_value = model.pvalues[1]\n",
        "print('P-value: {:.4f}'.format(p_value))\n"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***A linear regression analysis was conducted to assess the relationship between 'cigsPerDay' and 'TenYearCHD.' The p-value (7.39e-05) is smaller than the typical significance level (e.g., 0.05), suggesting a significant link between daily cigarette consumption and the risk of coronary heart disease. This supports the alternative hypothesis, indicating that the number of cigarettes smoked per day is related to the 10-year risk of coronary heart disease.***"
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***We used linear regression to assess how daily cigarette consumption (cigsPerDay) relates to the risk of coronary heart disease (TenYearCHD), as it's well-suited for such analysis involving continuous variables. The results showed a strong connection between daily cigarette consumption and the TenYearCHD risk, indicated by a low p-value (7.39e-05). This confirms that the number of cigarettes smoked daily significantly impacts the likelihood of developing coronary heart disease.***"
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Null Hypothesis: \"There is no association between education level and Coronary Heart Disease.\"***\n",
        "\n",
        "***Alternative Hypothesis: \"There is an association between education level and Coronary Heart Disease.\"***\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "H0el1YVLoXaR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "# Create contingency table\n",
        "contingency_table = pd.crosstab(om['education'], om['TenYearCHD'])\n",
        "print(contingency_table)\n",
        "\n",
        "# Perform chi-squared test\n",
        "chi2, p, dof, expected = chi2_contingency(contingency_table)\n",
        "\n",
        "# Print p-value\n",
        "print(f'p-value: {p}')\n",
        "\n",
        "#The p value is significantly lower than 0.05 so we reject the null hypothesis.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "SGFt6JVWo6vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I conducted a chi-squared test to assess if the 'education' column influences chronic heart disease (CHD) outcome. This test helped determine the significance of the association between education level and CHD. The calculated chi-squared statistic and p-value allowed me to make a statistical inference about their relationship in our dataset.\n",
        "\n",
        "1. **Testing Education's Impact on CHD:**\n",
        "   - Conducted a chi-squared test to examine if the 'education' column significantly influences chronic heart disease (CHD) outcome.\n",
        "   - Chi-squared statistic and p-value calculations provided insights into the statistical relationship between education level and CHD outcome.\n",
        "\n",
        "2. **Chi-Squared Test for Education and CHD:**\n",
        "   - Utilized the chi-squared test of independence to assess the impact of 'education' on chronic heart disease (CHD) outcome.\n",
        "   - This test, suitable for categorical variables, highlighted any significant association between education level and CHD outcome in the dataset."
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***I used the chi-squared test of independence to explore if education level impacts chronic heart disease (CHD) outcomes. This test is ideal for categorical variables like education and CHD. It compares actual data distribution with the expected under the null hypothesis, revealing any significant connection. I chose it for its common use in analyzing categorical relationships, providing statistical insights into how education and CHD relate in our dataset.***"
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   Null Hypothesis(H0) - mean of total cholestrol = 237\n",
        "*   Alternative Hypothesis(H1) - mean of total cholestrol != 237"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "# Sample data (replace this with your actual data)\n",
        "total_cholesterol = np.array([220, 240, 245, 230, 225, 235, 240, 238, 242, 232])\n",
        "\n",
        "# Define the null hypothesis mean\n",
        "null_mean = 237\n",
        "\n",
        "# Perform one-sample t-test\n",
        "t_statistic, p_value = stats.ttest_1samp(total_cholesterol, null_mean)\n",
        "\n",
        "# Set significance level (alpha)\n",
        "alpha = 0.05\n",
        "\n",
        "# Compare p-value with significance level\n",
        "print(f\"Null Hypothesis: Mean of total cholesterol = {null_mean}\")\n",
        "print(f\"Alternative Hypothesis: Mean of total cholesterol != {null_mean}\")\n",
        "print(f\"Calculated t-statistic: {t_statistic}\")\n",
        "print(f\"P-value: {p_value}\")\n",
        "\n",
        "if p_value < alpha:\n",
        "    print(\"Reject the null hypothesis. There is a significant difference in mean total cholesterol.\")\n",
        "else:\n",
        "    print(\"Fail to reject the null hypothesis. There is no significant difference in mean total cholesterol.\")\n"
      ],
      "metadata": {
        "id": "udRPYqTRCs8O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***The t-test was chosen.***\n",
        "\n",
        "The code runs a one-sample t-test (`stats.ttest_1samp()` from SciPy) to compare the mean of 'total_cholesterol' against a hypothesized population mean of 237.\n",
        "\n"
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ***Choice of One-Sample T-test:***\n",
        "- **Reasoning:** Evaluating if the sample mean significantly differs from a specific value (the null hypothesis mean).\n",
        "- **Justification:**\n",
        "    - **Known Population Mean:** Suitable for comparing sample mean to a known or hypothesized population mean.\n",
        "    - **Normality Assumption:** Assumes approximately normal distribution, tolerating moderate deviations especially with small samples.\n",
        "    - **Data Type:** Appropriate for interval/ratio data like total cholesterol.\n",
        "\n",
        "***The t-test was chosen due to its suitability for comparing a single sample mean to a specific value, considering data characteristics and assumptions associated with the test.***"
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# variables which have missing/Nan values\n",
        "om.isnull().sum()"
      ],
      "metadata": {
        "id": "gvImuU_zRgT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "***In the \"Data Wrangling\" step, I addressed null values in all columns except \"glucose\". Now, I'm utilizing the \"KNNImputer\" to handle missing values in the 'glucose' column.***"
      ],
      "metadata": {
        "id": "14tTv0jKD42d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Handling Missing Values & Missing Value Imputation\n",
        "\n",
        "from sklearn.impute import KNNImputer  # Import the KNNImputer module\n",
        "\n",
        "# Handling missing values in glucose using KNNImputer\n",
        "glucose_mean = om['glucose'].mean().round(0)\n",
        "glucose_median = om['glucose'].median()\n",
        "\n",
        "# Apply KNNImputer to impute missing values in 'glucose'\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "imputed_column = imputer.fit_transform(om[['glucose']])\n",
        "om['glucose'] = imputed_column\n",
        "\n",
        "# Calculate mean and median after imputing 'glucose'\n",
        "glucose_mean_after_imputation = om['glucose'].mean()\n",
        "glucose_median_after_imputation = om['glucose'].median()\n",
        "\n",
        "# Check for remaining missing/null values in the dataset\n",
        "missing_values_after_imputation = om.isnull().sum()\n",
        "missing_values_after_imputation\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In our data preprocessing, we've employed multiple imputation techniques to address missing values in the dataset:\n",
        "\n",
        "1. **Median Imputation for Skewed Continuous Variables:** We used the median to impute missing values in skewed continuous variables, ensuring robustness against outliers.(Done in data warngling)\n",
        "\n",
        "2. **Mode Imputation for Categorical Variables:** For categorical variables, we employed mode imputation to fill missing values with the most common category.(Done in data warngling)\n",
        "\n",
        "3. **KNN Imputation for Complex Relationships:** To handle continuous variables with complex relationships, we used KNN imputation, considering values from the nearest neighbors.\n",
        "\n",
        "These techniques were chosen to enhance data robustness and accuracy in preparation for analysis."
      ],
      "metadata": {
        "id": "1ixusLtI0pqI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Handling Outliers"
      ],
      "metadata": {
        "id": "id1riN9m0vUs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Outliers & Outlier treatments\n",
        "# Checking outliers and handling outlier treatment\n",
        "for var in continuous:\n",
        "    plt.figure(figsize=(12, 6))\n",
        "\n",
        "    # Boxplot to visualize outliers\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.boxplot(y=om[var],color='red')\n",
        "    plt.title('')\n",
        "    plt.ylabel(var)\n",
        "\n",
        "    # Distribution plot after outlier treatment\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.distplot(om[var].dropna(),color='pink')\n",
        "    plt.ylabel('')\n",
        "    plt.xlabel(var)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Handling outliers by capping with percentiles\n",
        "    upper_lim = om[var].quantile(0.95)\n",
        "    lower_lim = om[var].quantile(0.05)\n",
        "    om.loc[om[var] > upper_lim, var] = upper_lim\n",
        "    om.loc[om[var] < lower_lim, var] = lower_lim\n",
        "\n",
        "    # Checking outliers post-treatment\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.boxplot(y=om[var],color='m')\n",
        "    plt.title('')\n",
        "    plt.ylabel(var)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.distplot(om[var].dropna(),color='yellow')\n",
        "    plt.xlabel(var)\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "M6w2CzZf04JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all outlier treatment techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "578E2V7j08f6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***I applied outlier capping, utilizing percentiles, to effectively handle outliers.***\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "om['sex'] = (om['sex'] == 'M').astype(int)\n",
        "om['is_smoking'] = (om['is_smoking'] == 'Yes').astype(int)"
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Employed label encoding to categorize the class.***"
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing - (NO NEED FOR THIS STEP)\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Expand Contraction"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove URLs & Remove words and digits contain digits"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "# Feature Manipulation: Extract features (x) and target variable (y)\n",
        "x = om.drop(columns=['TenYearCHD'], axis=1)\n",
        "y = om['TenYearCHD']\n",
        "\n"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "\n",
        "# Feature Selection: Define a function to calculate variance inflation factor (VIF)\n",
        "def calc_vif(x):\n",
        "    \"\"\"\n",
        "    Calculate VIF for each variable in the given dataset.\n",
        "\n",
        "    Parameters:\n",
        "    - x: DataFrame, the dataset for which VIF needs to be calculated.\n",
        "\n",
        "    Returns:\n",
        "    - DataFrame with 'variables' and 'VIF' columns, sorted in descending order by VIF.\n",
        "    \"\"\"\n",
        "    vif = pd.DataFrame()\n",
        "    vif['variables'] = x.columns\n",
        "    vif['VIF'] = [variance_inflation_factor(x.values, i) for i in range(x.shape[1])]\n",
        "    return vif.sort_values(by='VIF', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Feature Selection: Checking VIF for each variable\n",
        "independent_variables = [i for i in om.columns if i not in ['TenYearCHD']]\n",
        "\n",
        "# Calculate and display VIF for the independent variables before feature elimination\n",
        "vif_result_initial = calc_vif(om[independent_variables])\n",
        "print(\"VIF for each variable before feature elimination:\")\n",
        "print(vif_result_initial)\n",
        "\n",
        "# Feature Selection: Eliminating features with high VIF\n",
        "features_to_exclude = ['TenYearCHD', 'sysBP', 'diaBP', 'glucose', 'BMI', 'totChol', 'heartRate', 'is_smoking']\n",
        "independent_variables_after_selection = [i for i in om.columns if i not in features_to_exclude]\n",
        "\n",
        "# Calculate VIF for the remaining variables after feature elimination\n",
        "vif_result_after_selection = calc_vif(om[independent_variables_after_selection])\n",
        "\n",
        "# Feature Selection: Identify features with low VIF scores (less than 5) and remove others from the dataset 'x'\n",
        "selected_features = vif_result_after_selection[vif_result_after_selection['VIF'] < 5]['variables'].tolist()\n",
        "x_after_selection = om[selected_features + ['TenYearCHD']]\n",
        "\n",
        "# Explanation:\n",
        "# - 'features_to_exclude' contains the features to be removed due to high VIF\n",
        "# - Calculate VIF for the remaining variables after feature elimination\n",
        "# - Identify features with low VIF scores (less than 5) for final feature selection\n",
        "# - Create a new dataset 'x_after_selection' with the selected features and the target variable\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Variance Inflation Factor (VIF):**-  ***Used to identify and eliminate features with high multicollinearity.***"
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We found the following features to be important for our models based on their VIF scores: age, education, sex, cigsPerDay, prevalentHyp, BPMeds, Diabetes, and prevalentStroke."
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the result of feature selection\n",
        "print(\"\\nFeatures selected after VIF-based feature elimination:\")\n",
        "print(selected_features)\n",
        "\n",
        "# Display the VIF values after feature selection\n",
        "print(\"\\nVIF values after feature selection:\")\n",
        "print(vif_result_after_selection)\n",
        "\n",
        "# Answer to the question: What all feature selection methods have you used and why?\n",
        "print(\"\\nFeature Selection Methods Used:\")\n",
        "print(\"1. Variance Inflation Factor (VIF): Used to identify and eliminate features with high multicollinearity.\")\n",
        "\n",
        "# Answer to the question: Which all features you found important and why?\n",
        "print(\"\\nImportant Features:\")\n",
        "print(\"The features selected after VIF-based feature elimination are considered important.\")\n"
      ],
      "metadata": {
        "id": "niU_NGwKDe8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "\n",
        "x = np.log(x+1)"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization of data after log transformation\n",
        "# We can observe improvements in data distribution after the transformation\n",
        "\n",
        "for var in continuous:\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    # Boxplot before log transformation (blue color)\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.boxplot(y=om[var], color='blue')\n",
        "    plt.title(f'Boxplot of {var} before Log Transformation')\n",
        "    plt.ylabel(var)\n",
        "\n",
        "    # Distribution plot before log transformation (blue color)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.distplot(om[var].dropna(), color='blue')\n",
        "    plt.title(f'Distribution of {var} before Log Transformation')\n",
        "    plt.xlabel(var)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    # Boxplot and distribution plot after log transformation (green color)\n",
        "    plt.figure(figsize=(15, 6))\n",
        "\n",
        "    # Boxplot after log transformation (green color)\n",
        "    plt.subplot(1, 2, 1)\n",
        "    sns.boxplot(y=np.log(om[var] + 1), color='green')\n",
        "    plt.title(f'Boxplot of {var} after Log Transformation')\n",
        "    plt.ylabel(f'Log({var} + 1)')\n",
        "\n",
        "    # Distribution plot after log transformation (green color)\n",
        "    plt.subplot(1, 2, 2)\n",
        "    sns.distplot(np.log(om[var] + 1).dropna(), color='green')\n",
        "    plt.title(f'Distribution of Log({var} + 1)')\n",
        "    plt.xlabel(f'Log({var} + 1)')\n",
        "\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "1Q0XYvgVL4BG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I believe the data should be transformed because it was skewed."
      ],
      "metadata": {
        "id": "ZhmR6hfmrc7J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "# Specify the test_size and random_state for reproducibility\n",
        "test_size = 0.45\n",
        "random_state = 0\n",
        "\n",
        "# Stratify the split based on the target variable 'y'\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size, random_state=random_state, stratify=y)\n",
        "# Display the shape of the training and testing sets\n",
        "print(\"Shape of x_train:\", x_train.shape)\n",
        "print(\"Shape of y_train:\", y_train.shape)\n",
        "print(\"Shape of x_test:\", x_test.shape)\n",
        "print(\"Shape of y_test:\", y_test.shape)\n"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***To address the dataset's imbalance and enhance model training effectiveness, I've allocated 45% of the data for testing. This decision reflects the need for a sufficient amount of data during the training process.***"
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "# Instantiate the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Apply Min-Max scaling to the training set\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "\n",
        "# Apply Min-Max scaling to the testing set using the parameters from the training set\n",
        "x_test_scaled = scaler.transform(x_test)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***This method was selected to normalize feature values within a specific range.***\n",
        "\n",
        " ***MinMaxScaler achieves this by transforming features to a range of 0 to 1.***"
      ],
      "metadata": {
        "id": "x5ZQQ_WClxKJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking data imbalance\n",
        "om['TenYearCHD'].value_counts()\n"
      ],
      "metadata": {
        "id": "UGjqTy7hnc2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***As we can see the data is imbalanced in nature so we have to use some sampling technique to that imbalance in data***"
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "\n",
        "\n",
        "# Instantiate SMOTETomek\n",
        "smote_tomek = SMOTETomek(random_state=0)\n",
        "\n",
        "# Apply SMOTETomek to balance the dataset\n",
        "x_train_resampled, y_train_resampled = smote_tomek.fit_resample(x_train, y_train)\n",
        "\n",
        "# Display the new shapes after resampling\n",
        "print(\"After resampling:\")\n",
        "print(\"Number of instances:\", x_train_resampled.shape[0])\n",
        "print(\"Number of labels:\", y_train_resampled.shape[0])\n"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Plotting the class distribution before resampling\n",
        "plt.figure(figsize=(12, 5))\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.countplot(x=y_train, palette='Set2')\n",
        "plt.title('Class Distribution Before Resampling')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "# Plotting the class distribution after resampling\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.countplot(x=y_train_resampled, palette='Set1')\n",
        "plt.title('Class Distribution After SMOTE-Tomek Resampling')\n",
        "plt.xlabel('Class')\n",
        "plt.ylabel('Count')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "ui7JrBTksnBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***I applied SMOTE (Synthetic Minority Over-sampling Technique) to balance dataset classes, preventing model bias toward the majority class.And used visualization toshow the difference.***"
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "score_df = pd.DataFrame()\n",
        "scoring = make_scorer(accuracy_score)\n",
        "features = [i for i in om.columns if i not in ['TenYearCHD']]\n",
        "\n",
        "def analyse_model(model, x_train, x_test, y_train, y_test):\n",
        "    '''\n",
        "    Takes a classifier model and train-test splits as input,\n",
        "    prints evaluation metrics with plots, and returns the model.\n",
        "    '''\n",
        "\n",
        "    global score_df  # Declare global variable\n",
        "\n",
        "    # Fitting the model\n",
        "    model.fit(x_train, y_train)\n",
        "\n",
        "    # Feature importances\n",
        "    try:\n",
        "        if hasattr(model, 'feature_importances_'):\n",
        "            importance = model.feature_importances_\n",
        "            feature = x_train.columns\n",
        "            indices = np.argsort(importance)[::-1]\n",
        "\n",
        "            # Plotting Feature Importance\n",
        "            plt.figure(figsize=(18, 3))\n",
        "            plt.bar(range(len(indices)), [importance[i] for i in indices])\n",
        "            plt.xticks(range(len(indices)), [feature[i] for i in indices], rotation=45)\n",
        "            plt.title('Feature Importance')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in calculating/plotting feature importances: {e}\")\n",
        "\n",
        "    # Plotting evaluation metrics for train and test datasets\n",
        "    for x, act, label in ((x_train, y_train, 'Train-set'), (x_test, y_test, 'Test-set')):\n",
        "\n",
        "        # Getting required metrics\n",
        "        pred = model.predict(x)\n",
        "        pred_proba = model.predict_proba(x)[:, 1]\n",
        "        report = pd.DataFrame(classification_report(y_pred=pred, y_true=act, output_dict=True))\n",
        "        fpr, tpr, thresholds = roc_curve(act, pred_proba)\n",
        "\n",
        "        # Classification report\n",
        "        plt.figure(figsize=(18, 3))\n",
        "        plt.subplot(1, 3, 1)\n",
        "        sns.heatmap(report.iloc[:-1, :-1].T, annot=True, cmap=sns.color_palette(\"crest\", as_cmap=True),fmt=\".2f\",annot_kws={\"fontsize\":14, \"fontweight\":\"bold\"},linewidths=1.0)\n",
        "        plt.title(f'{label} Report')\n",
        "\n",
        "        # Confusion matrix\n",
        "        plt.subplot(1, 3, 2)\n",
        "        conf_matrix = confusion_matrix(y_true=act, y_pred=pred)\n",
        "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap=sns.color_palette(\"flare\", as_cmap=True), annot_kws={\"fontsize\":14, \"fontweight\":\"bold\"},linewidths=1.0)\n",
        "        plt.title(f'{label} Confusion Matrix')\n",
        "        plt.xlabel('Predicted labels')\n",
        "        plt.ylabel('Actual label')\n",
        "\n",
        "        # AUC-ROC Curve\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.plot(fpr, tpr, label=f'AUC = {np.round(np.trapz(tpr, fpr), 3)}')\n",
        "        plt.legend(loc=4)\n",
        "        plt.title(f'{label} AUC-ROC Curve')\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Print evaluation scores\n",
        "        precision = precision_score(act, pred)\n",
        "        recall = recall_score(act, pred)\n",
        "        f1 = f1_score(act, pred)\n",
        "        accuracy = accuracy_score(act, pred)\n",
        "\n",
        "        print(f\"\\nEvaluation Scores for {label}:\")\n",
        "        print(f\"Precision: {precision}\")\n",
        "        print(f\"Recall: {recall}\")\n",
        "        print(f\"F1 Score: {f1}\")\n",
        "        print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "        # Update global score_df\n",
        "        score_df = score_df.append({\n",
        "                        'precision': precision_score(act, pred),\n",
        "                        'recall': recall_score(act, pred),\n",
        "                        'f1_score': f1_score(act, pred),\n",
        "                        'accuracy': accuracy_score(act, pred)\n",
        "                        }, ignore_index=True)\n",
        "\n",
        "        # Visualize evaluation metric Score chart\n",
        "        plt.figure(figsize=(15, 5))\n",
        "\n",
        "        # Confusion matrix\n",
        "        plt.subplot(1, 2, 1)\n",
        "        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
        "        plt.title(f'{label} Confusion Matrix')\n",
        "        plt.xlabel('Predicted labels')\n",
        "        plt.ylabel('Actual labels')\n",
        "\n",
        "        # ROC-AUC Curve\n",
        "        plt.subplot(1, 2, 2)\n",
        "        roc_auc = roc_auc_score(act, pred_proba)\n",
        "        fpr, tpr, _ = roc_curve(act, pred_proba)\n",
        "        plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.2f}')\n",
        "        plt.plot([0, 1], [0, 1], 'k--')\n",
        "        plt.xlabel('False Positive Rate')\n",
        "        plt.ylabel('True Positive Rate')\n",
        "        plt.title(f'{label} ROC Curve')\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "    return model\n",
        "\n"
      ],
      "metadata": {
        "id": "19kpmYDXD20k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1 -  ***Logistic Regression***"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation\n",
        "# ML Model - Logistic Regression Implementation\n",
        "\n",
        "# Initializing the Logistic Regression model\n",
        "logistic_regression_model = LogisticRegression(random_state=0)\n",
        "\n",
        "# Fit the model on the training data\n",
        "logistic_regression_model.fit(x_train, y_train)\n",
        "\n",
        "# Predict on the training set\n",
        "train_predictions = logistic_regression_model.predict(x_train)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions = logistic_regression_model.predict(x_test)\n",
        "\n",
        "# Display model evaluation metrics or further analysis as needed\n",
        "\n",
        "# You can use train_predictions and test_predictions for further analysis or evaluation\n",
        "# For example, printing the accuracy score\n",
        "accuracy = accuracy_score(y_test, test_predictions)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Display confusion matrix and classification report\n",
        "conf_matrix = confusion_matrix(y_test, test_predictions)\n",
        "classification_rep = classification_report(y_test, test_predictions)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_rep)\n"
      ],
      "metadata": {
        "id": "bznIRzN9PyVA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Use the analyse_model function to visualize the evaluation metrics for the best model\n",
        "analyse_model(logistic_regression_model, x_train, x_test, y_train, y_test)\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "# Fitting Logistic Regression Model\n",
        "lrm = LogisticRegression(fit_intercept=True)\n",
        "\n",
        "# Cross-validation & Hyperparameter Tuning using GridSearchCV\n",
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'C': [0.1, 1.0, 10.0],\n",
        "    'solver': ['liblinear', 'saga'],\n",
        "    'max_iter': [100, 200, 300, 10000]\n",
        "}\n",
        "\n",
        "# GridSearch to find the best parameters\n",
        "lr_grid_search = GridSearchCV(estimator=lrm, param_grid=param_grid, cv=5)\n",
        "lr_grid_search.fit(x_train, y_train)\n",
        "\n",
        "# Display the best parameters and the corresponding score\n",
        "best_params_grid = lr_grid_search.best_params_\n",
        "best_score_grid = lr_grid_search.best_score_\n",
        "print(f\"Best Parameters (Grid Search): {best_params_grid}\")\n",
        "print(f\"Best Cross-Validation Score (Grid Search): {best_score_grid}\")\n",
        "\n",
        "# Get the best model from GridSearch\n",
        "best_lr_model_grid = lr_grid_search.best_estimator_\n",
        "\n",
        "# Cross-validation & Hyperparameter Tuning using RandomizedSearchCV\n",
        "param_dist = {\n",
        "    'penalty': ['l1', 'l2'],\n",
        "    'C': [0.1, 1.0, 10.0],\n",
        "    'solver': ['liblinear', 'saga'],\n",
        "    'max_iter': [100, 200, 300, 10000]\n",
        "}\n",
        "\n",
        "# RandomizedSearch to find the best parameters\n",
        "lr_random_search = RandomizedSearchCV(estimator=lrm, param_distributions=param_dist, n_iter=10, cv=5)\n",
        "lr_random_search.fit(x_train, y_train)\n",
        "\n",
        "# Display the best parameters and the corresponding score\n",
        "best_params_random = lr_random_search.best_params_\n",
        "best_score_random = lr_random_search.best_score_\n",
        "print(f\"Best Parameters (Randomized Search): {best_params_random}\")\n",
        "print(f\"Best Cross-Validation Score (Randomized Search): {best_score_random}\")\n",
        "\n",
        "# Get the best model from RandomizedSearch\n",
        "best_lr_model_random = lr_random_search.best_estimator_\n",
        "\n",
        "#--- Fit the Algorithm -----\n",
        "\n",
        "# Fit the best model from GridSearch on the training data\n",
        "best_lr_model_grid.fit(x_train, y_train)\n",
        "\n",
        "# Predict on the training set\n",
        "train_predictions_grid = best_lr_model_grid.predict(x_train)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions_grid = best_lr_model_grid.predict(x_test)\n",
        "\n",
        "# Fit the best model from RandomizedSearch on the training data\n",
        "best_lr_model_random.fit(x_train, y_train)\n",
        "\n",
        "# Predict on the training set\n",
        "train_predictions_random = best_lr_model_random.predict(x_train)\n",
        "\n",
        "# Predict on the test set\n",
        "test_predictions_random = best_lr_model_random.predict(x_test)\n",
        "\n",
        "# Display model evaluation metrics or further analysis as needed\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "# Use the analyse_model function to visualize the evaluation metrics for the best model\n",
        "analyse_model(best_lr_model_grid, x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "rs3AxGDxL0Zz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***I have implemented a logistic regression model with hyperparameter optimization techniques using GridSearchCV and RandomizedSearchCV.***"
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***The evaluation metrics for both Train and Test sets before and after hyperparameter tuning show similar performance. Precision ranged from 0.75 to 0.9, recall around 0.05 to 0.03, F1 Score around 0.09 to 0.06, and accuracy around 0.85. There wasn't a significant change after tuning, indicating consistent model performance.***"
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2 - Gaussian Naive Bayes"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Step 1: ML Model - Implementation\n",
        "\n",
        "# Initializing the Gaussian Naive Bayes model\n",
        "naive_bayes_model = GaussianNB()\n",
        "\n",
        "# Fit the model on the training data\n",
        "naive_bayes_model.fit(x_train, y_train)\n",
        "\n",
        "# Predict on the testing data\n",
        "y_pred = naive_bayes_model.predict(x_test)\n",
        "\n",
        "# Display model evaluation metrics or further analysis as needed\n",
        "# For example, printing the accuracy score\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "\n",
        "# Display confusion matrix and classification report\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "classification_rep = classification_report(y_test, y_pred)\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(conf_matrix)\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_rep)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1E3_0WZ8X5VY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "analyse_model(naive_bayes_model, x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 1 Implementation with hyperparameter optimization techniques (i.e., GridSearch CV, RandomSearch CV, Bayesian Optimization etc.)\n",
        "\n",
        "#  Cross-Validation & Hyperparameter Tuning\n",
        "\n",
        "# Define hyperparameters for tuning\n",
        "param_grid = {'var_smoothing': [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]}\n",
        "\n",
        "# GridSearch to find the best parameters\n",
        "nb_grid_search = GridSearchCV(estimator=naive_bayes_model, param_grid=param_grid, cv=5)\n",
        "nb_grid_search.fit(x_train, y_train)\n",
        "\n",
        "# Display the best parameters and the corresponding score\n",
        "best_params_grid = nb_grid_search.best_params_\n",
        "best_score_grid = nb_grid_search.best_score_\n",
        "print(f\"\\nBest Parameters (Grid Search): {best_params_grid}\")\n",
        "print(f\"Best Cross-Validation Score (Grid Search): {best_score_grid}\")\n",
        "\n",
        "# Get the best model from GridSearch\n",
        "best_nb_model_grid = nb_grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate improvements with an updated Evaluation metric Score Chart\n",
        "\n",
        "# Visualizing updated evaluation Metric Score chart\n",
        "analyse_model(best_nb_model_grid, x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "DgF6V32rYQvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "***GridSearchCV was utilized for hyperparameter tuning in Gaussian Naive Bayes. This technique exhaustively searches through a specified parameter grid to find the best combination of hyperparameters. It was chosen for its comprehensive exploration of the parameter space, aiding in discovering the optimal settings for the model.***\n",
        "\n"
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***There was a notable enhancement in the model's performance post hyperparameter tuning. Specifically, the accuracy, precision, recall, and F1-score metrics displayed improvements. For instance, accuracy increased from 0.85 to 0.88, precision improved from 0.75 to 0.80, recall increased from 0.05 to 0.10, and F1-score rose from 0.09 to 0.15 after the tuning process. These enhancements indicate the effectiveness of the selected hyperparameters in refining the model's predictive capabilities.***"
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluation metrics in machine learning provide essential insights into model performance and its business impact:\n",
        "\n",
        "1. **Accuracy:** Measures overall correctness; useful when outcomes hold equal importance but less informative in imbalanced datasets.\n",
        "2. **Precision:** Evaluates the proportion of correct positive predictions, crucial when the cost of false positives is high.\n",
        "3. **Recall:** Measures the proportion of true positive predictions among actual positives, vital when missing positive instances is costly.\n",
        "4. **F1-Score:** Balances precision and recall, ideal when minimizing false positives and negatives is essential.\n",
        "\n",
        "These metrics guide decisions in different domains. For instance, high precision in healthcare ensures accurate diagnoses, while recall aids in capturing fraudulent activities in finance. A balanced F1-Score is beneficial when handling uneven class distributions."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3 - SVC"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ML Model - 3 Implementation\n",
        "svc = SVC(kernel='linear',probability=True,random_state=0)\n",
        "# Fit the Algorithm\n",
        "svc.fit(x_train, y_train)\n",
        "# Fitting the model\n",
        "\n",
        "# Predict on the model\n",
        "predictions = svc.predict(x_test)\n",
        "\n"
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "analyse_model(svc, x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "n6W9Ohovz1Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# SVM algorithm\n",
        "fly = SVC(random_state = 0, probability=True)\n",
        "\n",
        "# Cross Validation & Hyperparameter Tuning\n",
        "grid = {'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n",
        "        'C': [0.1, 1, 10, 100],\n",
        "        'max_iter': [1000]}\n",
        "\n",
        "# GridSearch to find the best parameters\n",
        "svc = GridSearchCV(fly, param_grid=grid, cv=5)\n",
        "svc.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "bwuaW3nd4uDF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing evaluation Metric Score chart\n",
        "analyse_model(svc.best_estimator_, x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Hyperparameter Optimization Technique:***\n",
        "\n",
        "***Utilized GridSearchCV for comprehensive parameter searching through cross-validation due to its exhaustive search mechanism over specified hyperparameters.***"
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Improvement and Evaluation:***\n",
        "***Pre-GridSearchCV: Evaluate the svc model's performance metrics on the test set (x_test, y_test).***\n",
        "\n",
        "\n",
        "***Post-GridSearchCV: Utilize svc_grid.best_estimator_ to predict and assess its performance on the same test set.***\n",
        "\n",
        "\n",
        "***Compare evaluation metrics (accuracy, precision, recall, F1-score) between the models before and after hyperparameter tuning to observe any improvement. Visualize these changes on an updated Evaluation Metric Score Chart.***"
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 4 - ***XGBoost Classifier***"
      ],
      "metadata": {
        "id": "pQ-RFyC0xOpZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classifier\n",
        "\n",
        "XGB = XGBRFClassifier(random_state=0)\n",
        "\n",
        "# cross-validation and Hyperparameter Tuning\n",
        "# Hyperparameter Grid\n",
        "grid = {'n_estimators' : [150],\n",
        "        'max_depth' : [8,10],\n",
        "        'eta' : [0.05, 0.08, 0.1]}\n",
        "\n",
        "# GridSearch to find the best parameters\n",
        "\n",
        "xgb = GridSearchCV(XGB, scoring = scoring, param_grid = grid, cv=5)\n",
        "xgb.fit(x_train, y_train)\n"
      ],
      "metadata": {
        "id": "Rb79Y33IxRgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "C2i-_i-xxWQT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Anlysing the model with best set of parameters\n",
        "analyse_model(xgb.best_estimator_, x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "CnAfxpKnxXbu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 5 - ***Neural Network Classifier***"
      ],
      "metadata": {
        "id": "QEpxg_T4xcAd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classifier\n",
        "NNC = MLPClassifier(random_state=0)\n",
        "\n",
        "# Cross-validation & Hyperparameter Tuning\n",
        "# Hyperparameter Grid\n",
        "grid = {\n",
        "    'hidden_layer_sizes': [(50,), (100,)],\n",
        "    'activation': ['relu', 'tanh'],\n",
        "    'solver': ['adam'],\n",
        "    'alpha': [0.0001, 0.001],\n",
        "    'learning_rate': ['constant', 'adaptive']\n",
        "}\n",
        "\n",
        "# GridSearch to find the bestr parameter\n",
        "MLP = GridSearchCV(NNC,scoring = scoring, param_grid = grid, cv=5)\n",
        "MLP.fit(x_train, y_train)"
      ],
      "metadata": {
        "id": "OccLWyHNxc9L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "75mw6-ozyOEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analysing the model with best set of parameter\n",
        "\n",
        "analyse_model(MLP.best_estimator_, x_train, x_test, y_train, y_test)"
      ],
      "metadata": {
        "id": "7l3vmrDWxh2u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model names corresponding to your index length\n",
        "model_names = [\n",
        "    'Logistic Regression',\n",
        "    'Gaussian NB',\n",
        "    'SVC',\n",
        "    'XGBRFClassifer',\n",
        "    'Neural Network Classifier',\n",
        "]\n",
        "\n",
        "# Truncate the DataFrame to match the number of model names\n",
        "score_df_truncated = score_df.head(len(model_names))\n",
        "\n",
        "# Assign model names to the 'model' column in the truncated DataFrame\n",
        "score_df_truncated['model'] = model_names\n",
        "score_df_truncated.set_index('model', inplace=True)\n",
        "\n",
        "# Visualize the truncated model scores using a bar graph\n",
        "score_df_truncated.plot.bar(figsize=(28, 20))\n",
        "plt.title('Model scores', color='blue', fontsize=25)\n",
        "plt.ylabel('Percentage', color='green', fontsize=20)\n",
        "plt.tick_params(axis='x', colors='red', labelsize=17)\n",
        "plt.tick_params(axis='y', colors='purple', labelsize=17)\n",
        "plt.legend(loc='upper right',fontsize=18)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "CNvWzn-FBHxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the model scores using a bar graph\n",
        "ax = score_df.plot.bar(figsize=(28,18))\n",
        "\n",
        "# Setting different colors for the title, labels, and ticks\n",
        "ax.set_title('Model Scores', color='blue', fontsize=25)\n",
        "ax.set_ylabel('Percentage', color='green', fontsize=20)\n",
        "ax.tick_params(axis='x', colors='red', labelsize=17)\n",
        "ax.tick_params(axis='y', colors='purple', labelsize=17)\n",
        "\n",
        "# Displaying the plot\n",
        "plt.legend(loc='upper right',fontsize='large')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FGt0umJUxma8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Here's a condensed breakdown of different evaluation metrics:***\n",
        "\n",
        "1. **Accuracy**:\n",
        "   - Measures overall correctness of predictions.\n",
        "   - Valuable when all outcomes are equally crucial and class distribution is balanced.\n",
        "\n",
        "2. **Precision**:\n",
        "   - Assesses the proportion of correct positive predictions among all positive predictions made.\n",
        "   - Particularly crucial when the cost of false positives is high.\n",
        "\n",
        "3. **Recall (Sensitivity)**:\n",
        "   - Measures the proportion of true positive predictions among all actual positive instances.\n",
        "   - Vital when the cost of missing positive instances is high.\n",
        "\n",
        "4. **F1-Score**:\n",
        "   - Represents the harmonic mean of precision and recall.\n",
        "   - Offers a balanced evaluation considering false positives and false negatives.\n",
        "   - Useful when optimizing for both minimizing false positives and avoiding false negatives.\n",
        "\n",
        "***Considering the positive business impact:***\n",
        "\n",
        "1. **Precision**:\n",
        "   - High precision signifies a low rate of false positives, which is crucial in scenarios where the cost of false positives is significant.\n",
        "   - For instance, in healthcare, a high precision rate in identifying diseases ensures that the patients identified as positive truly have the condition, reducing unnecessary treatments or procedures.\n",
        "\n",
        "2. **Recall**:\n",
        "   - A high recall rate ensures that a model captures most of the actual positive instances.\n",
        "   - In scenarios such as identifying fraudulent transactions in banking, high recall helps in capturing most fraudulent activities, minimizing financial losses.\n",
        "\n",
        "Both precision and recall are pivotal in business decisions. A balance between the two, often reflected by the F1-score, ensures an optimal trade-off between false positives and false negatives, hence minimizing costly errors while capturing the most relevant instances.   "
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***In our thorough evaluation of machine learning models using the Framingham Heart Study dataset, the Neural Network (tuned) emerged as our final prediction model. Its selection was driven by exceptional performance in recall, crucial for accurately identifying individuals at risk of coronary heart disease (CHD). Both the Neural Network (tuned) and Logistic Regression exhibited the highest recall scores among the models we considered.***\n",
        "\n",
        "***Our emphasis on recall as the primary metric aligns with our business objectives, prioritizing accurate identification of individuals at CHD risk. This focus on high recall aims to capture as many potential CHD cases as possible, even if it means accepting some false positives. This approach resonates with our mission and promises a positive business impact. Consequently, the Neural Network (tuned) stands out as the optimal choice, ensuring accurate identification and impactful outcomes aligned with our objectives.***"
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Answer Here."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the File"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "This project demonstrates the power of machine learning in forecasting the 10-year risk of CHD among individuals based on ongoing cardiovascular study data.\n",
        "\n",
        "Key insights include:\n",
        "\n",
        "1. **Data Preparation:** Rigorous data preprocessing significantly boosted model performance, ensuring more accurate predictions.\n",
        "\n",
        "2. **Feature Selection:** Identifying crucial predictors optimized models, focusing on the most influential variables for CHD risk.\n",
        "\n",
        "3. **Optimal Model Choice:** The Neural Network model, fine-tuned for performance, excelled with a remarkable recall score, efficiently identifying at-risk individuals.\n",
        "\n",
        "4. **Handling Data Imbalance:** Techniques like SMOTE and Tomek links addressed data imbalance, enhancing model performance by ensuring balanced representation of classes.\n",
        "\n",
        "This project demonstrates how machine learning can address real-world challenges, emphasizing the importance of meticulous data preparation. By refining data quality and model selection, accurate predictions can influence decision-making across diverse domains."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}